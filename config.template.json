{
    "pipeline": {
        "log_level": "INFO",
        "log_file": "sketch_to_mermaid_feedback.log"
    },
    "input_parser": {
        "ocr_confidence_threshold": 0.3,
        "mandatory_ocr": true,
        "use_backup_ocr": true
    },
    "vlm_engine": {
        "model_name": "Qwen/Qwen-VL-Chat",
        "tensor_parallel_size": 1,
        "gpu_memory_utilization": 0.9,
        "max_tokens": 1000,
        "temperature": 0.1,
        "fallback_enabled": true
    },
    "mermaid_generator": {
        "llm_provider": "gemini",
        "gemini_api_key": "YOUR_GEMINI_API_KEY_HERE",
        "gemini_model": "gemini-2.5-flash",
        "ollama_url": "http://localhost:11434",
        "ollama_model": "qwen2.5vl:latest",
        "temperature": 0.3,
        "timeout": 120,
        "fallback_enabled": true,
        "use_intelligent_analysis": true
    },
    "image_generator": {
        "provider": "gemini",
        "comment": "NOTE: This section is for legacy Vertex AI Imagen. Current implementation uses Gemini native image generation (gemini-2.5-flash-image) which requires no additional config beyond the gemini_api_key above."
    },
    "_instructions": {
        "setup": [
            "1. Copy this file to config.json",
            "2. Get your Gemini API key from https://aistudio.google.com/app/apikey",
            "3. Replace YOUR_GEMINI_API_KEY_HERE with your actual API key",
            "4. (Optional) Configure Ollama fallback if you want offline support"
        ]
    }
}
